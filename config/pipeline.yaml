# config/pipeline.yaml

# 1. Where your raw sequencing reads live
reads_dir: "/Users/utente/Desktop/PhD/Tuber/raw_reads"

# 2. Taxonomy spreadsheet (Excel .xlsx or .csv BOLD format)
taxonomy_file: "/Users/utente/Downloads/bold_taxonomy.xlsx"
taxonomy_sheet:   "Taxonomy" #Taxonomy sheet of the spreadsheet

# 3. Reference database (FASTA of SSU, ITS, LSU sequences) # ref_fasta is auto-downloaded if unset
ref_fasta: "" 

# 4. Path to Trimmomatic adapter file
trimmomatic_adapters: auto # The pipeline will automatically locate the adapters in the ssuitslsu-trimmomatic Conda env
skip_trimming:   false # if true, will *NOT* run Trimmomatic but use existing *_trimmed_1P/2P.fastq.gz

# 5. coverage check
auto_subsample: true       # always check coverage and subsample if needed
max_coverage: 100          # threshold (×) above which to downsample
target_coverage: 90        # aim (×) when subsampling

# 6. Assembler
assembler: spades    # change to "spades" to run SPAdes instead

# 7. Computational resources
threads: 8       # e.g. number of CPU cores to use
mem_gb: 16       # e.g. total RAM in GB

# 8. Where to put all output
outdir: "results"